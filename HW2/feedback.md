# 1.

Производная посчитана правильно! Так держать :)

Градиентный спуск тоде правильно!

На самом деле, в функции с градиентным спуском можно было обойтись и без копирования, потому что потом сразу же `curr_w` будет ссылаться уже на другой элемент (после строчки с шагом спуска `curr_w -= lr * grad_f(curr_w)`)


*Вопрос на засыпку*: попали ли мы в глобальный минимум функции? Сколько у функии `f` вообще минимумов и максимумов?

# 2

## 2.1

По поводу `mse` дал комментарии в коде - там можно проще посчитать, твой вариант тоже правильный, но более читаемо и эквавалентно просто посчиать разность, а потом возвезти квадрат. Но согласен, что на языке линейной алгебры это делается как у тебя в коде. Еще я бы не использовал `np.size` в данно ключе - понятно, что у тебя размерность вектора равна размеру, так как это вектор-столбец, но в других задачах можно запутаться. Достаточно `len(y_true)` или `y_true.shape[0]`. И тут сразу можно воспользоваться `np.mean`.

Для массивов с двумя размерностями можно использовать синтаксический сахар и транспонирование делать как `X.T`, а перемножение матриц как `A @ B` к примеру.

Отдельный респект за то, что все константы выносишь в начало выражения и берешь в скобочки - и читемо, и помогает в вычислительной стабильности.

## 2.2

К методу линейной регрессии вопросов нет! Все супер.

## 2.3

mse дублируется как имя переменной и название функции! Не, так лучше не делать - надоест потом ошибку искать. вынужден снять профилактические четверть балла. А за пайплайн бонус в 0.25 баллов ты получаешь :)

Единственное - пиши что за цифры ты выводишь - боссу/заказчику пофиг на код, но цифры надо все пояснять, как и вообще все выводы в консоль/принты в stdout/вывод в файлик! Вон у spades каке логи подробные!

# 3

## 3.1

К сигмоиде вопросов нет!

## 3.2

Отличная идея изначально предотвратить взрыв в логарифме!  Но зачем клипать сверху? Это лишнее, у тебя иначе никогда не будет `y_pred=1`. Понятно что он стремится к 1, но ты его клипаешь все равно. Хотя `1e-15` не страшно. 

Можно делать по-другому - можно не клипать `y_pred`, а просто добавлять сразу `eps` в логарифм (показал в коде). Я поставлю доп полбалла за рассмотрение корнер кейса! Good job :)

Еще забыла усреднить



## 3.3

Класс реализовала - все верно! C метриками тоже все супер, сделала. ПИШИ ЧТО ВЫВОДИШЬ ТОЛЬКО!)) Четвертинку снимаю

## 3.4

Ты видимо после оформления ноутбука оформила иерархию папок и положила данные в отдельную папку, но в коду осталась их прошлая локализация, поэтому `FileNotFoundError`. Недосмотрела - проверяй вседа работу после всех изменений! Особенно в полноценной рабоет это оччень важно! Снимаю четверть балла за это, забыла проверить работоспособность кода.

Вывод уникальных значений достаточно сумбурный, тут лучше бы подошла проверка на то, какой вид фичей имеем - категориальные, вещественные или булевы + гистограммы, еще можно сделать корреляционную матрицу.

За пайплайн - супер идея - +0.5 баллов!

Еще момент - как обычно водится, в ml есть устоявшиеся аббревиатура, и `lr` закреплено за `learning_rate` - поэтому ээто не придирка, но вводит в ступор, что у тебя логистическая регрессия так названа :) Это с опытом придет, лучше тут написать `log_reg`.

Confusion matrix супер

Как нам посмотреть feature importance? Можно удалять фичи и смотреть, какие при удалении снижают скор и насколько. Можно посмортеть на коэффициенты, которые получились - если они высокие и ненулевые - то можно говорить, что для модели они важны, в какой-то степени. На это смотрят, но не часто.

# 4

kraftwerk классные! Слушал у них некоторые песни, помню, расстроился, когда узнал, что у них Шнайдер умер (

ИТОГ:

1 - 1/1
2.1 - 2/2
2.2 - 3/3
2.3 - 4/4 + 0.5 (доп за корнер кейс)
3.1 - 0.5/0.5
3.2 - 2.25/2.5
3.3 - 1.75/2
3.4 - 5/5 + 0.5
4 - 0.5/0.5


СУММА: 19.5 + 1.5 доп = 21 балл * 0.8 коэфф = 16.8

Работа у тебя получилась очень хорошая, так держать! Местами можно упрощать код за счет использованя синтаксического сахара нампая, но в остальном все супер. Еще делай более широкий и понятный EDA!.